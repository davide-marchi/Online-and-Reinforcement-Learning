\subsection{Part 1}

Evaluating algorithms for online learning with limited feedback in real-life scenarios is challenging. 
While the most direct approach is to implement an algorithm and measure its performance live, this is often not feasible due to:

\begin{itemize}
    \item \textbf{Potential risks, costs, and time delays.} Deploying a potentially suboptimal algorithm can lead to high financial or reputational costs. Moreover, once the algorithm is running, it takes time to collect sufficient data for analysis, which delays insights and can be inefficient if the algorithm underperforms.

    \item \textbf{Difficulty of controlled experimentation.} Once data is collected based on a particular algorithm's actions, it is nearly impossible to ``replay'' the same conditions to test different algorithms under identical circumstances. This lack of controlled repetition makes fair comparisons difficult.
\end{itemize}

\subsection{Part 2}

\noindent
\subsubsection*{(a) Modification of UCB1 for Importance-Weighted Losses (Uniform Sampling).}

\medskip
To handle partial feedback when arms are chosen uniformly at random (probability $1/K$ for each arm), we replace the usual empirical loss estimates in UCB1 with importance-weighted estimates. Concretely, whenever an arm $i$ is chosen, its observed loss is scaled by the factor $\tfrac{1}{p_i(t)} = K$, ensuring an unbiased estimator. The main changes from standard UCB1 are thus:
\begin{itemize}
    \item \textbf{Empirical Loss Update:} Instead of adding the raw observed loss $\ell_{i,t}$, we add $K \cdot \ell_{i,t}$ to the running total for arm $i$. 
    \item \textbf{Confidence Bounds:} The increased variance (due to multiplying by $K$) is accounted for in the confidence term, typically by a constant factor in front of the usual $\sqrt{\frac{\ln t}{N_i(t)}}$ bound. 
\end{itemize}

\medskip
\textbf{Pseudo-Code of the Modified Algorithm:}

\bigskip
\noindent
\textbf{Initialize:} For each arm $i \in \{1,\dots,K\}$,
\[
    \widehat{L}_i(0) = 0,\quad N_i(0) = 0.
\]

\medskip
\noindent
\textbf{For} $t = 1$ \textbf{to} $T$:
\begin{enumerate}
    \item Select arm 
    \[
        A_t \;=\; \arg\min_{i \in \{1,\dots,K\}} \Bigl(\widehat{L}_i(t-1) \;+\; c \,\sqrt{\tfrac{\ln(t-1)}{N_i(t-1)}}\Bigr),
    \]
    where $c$ is a positive constant (improved parameter choice).
    \item Observe the loss $\ell_{A_t,t}$ for the chosen arm $A_t$.
    \item Update counts:
    \[
        N_{A_t}(t) \;=\; N_{A_t}(t-1) + 1,\quad N_j(t) = N_j(t-1)\;\;\text{for } j \neq A_t.
    \]
    \item Update the empirical loss estimate of arm $A_t$ via importance weighting:
    \[
        \widehat{L}_{A_t}(t) \;=\; \frac{N_{A_t}(t-1)\,\widehat{L}_{A_t}(t-1)\;+\;K \,\ell_{A_t,t}}{N_{A_t}(t)},
    \]
    and keep $\widehat{L}_j(t) = \widehat{L}_j(t-1)$ for $j \neq A_t$.
\end{enumerate}

\bigskip
\noindent
\textbf{Key Changes and Regret Bound:} 
\begin{itemize}
    \item The use of $K \cdot \ell_{A_t,t}$ ensures an unbiased estimate of the true mean loss for arm $A_t$ under uniform sampling ($p_i(t) = 1/K$).
    \item Accounting for the variance increase (due to multiplication by $K$) requires adjusting the confidence term by an appropriate constant factor $c$. 
    \item The pseudo-regret analysis follows similarly to the standard UCB1 derivation, with an additional factor from the importance weighting. The resulting pseudo-regret remains on the order of 
    \[
        O\bigl(\sqrt{K\,T \ln T}\bigr),
    \]
    up to constants that depend on $c$ and problem-specific parameters.
\end{itemize}

\noindent
\subsubsection*{(b) Why the Modified UCB1 Cannot Exploit the Small Variance}

In the modified UCB1, each observed loss is multiplied by $K$ (because of the importance-weighting factor $1/p_i(t)$ with $p_i(t) = 1/K$). 
Even if the original loss distribution has small variance, the algorithm's effective variance is dominated by the extra factor of $K$. 
UCB1 constructs confidence intervals by considering worst-case deviations of these estimates. 
Hence, the potential benefit of a small underlying variance cannot be directly exploited; 
the importance-weighted updates inflate the variance term in the confidence bounds, 
making the algorithm behave more conservatively (as if the variance were larger).

\bigskip
\noindent
\subsubsection*{(c) Modifying EXP3 for Importance-Weighted Losses (Uniform Sampling)}

We consider a logging policy that selects arms uniformly at random, 
with probability $1/K$ for each arm. 
We modify the EXP3 algorithm so that, whenever the logging policy selects an arm $A_t$, 
we form an unbiased estimate of its loss by scaling the observed loss by $K$. 
The pseudo-code follows closely the standard EXP3 structure but includes an importance-weighted update:

\bigskip
\noindent
\textbf{Pseudo-code for Modified EXP3:}

\begin{enumerate}
    \item \textbf{Initialization:} 
    \[
        \text{For each arm } i \in \{1,\dots,K\},\quad w_i(1) = 1.
    \]
    \item \textbf{For each round } $t = 1,2,\dots,T$:
    \begin{enumerate}
        \item \textbf{(Logging policy)} An arm $A_t$ is chosen \emph{uniformly at random} with probability $\frac{1}{K}$.
        \item \textbf{(Observe loss)} Observe the loss $\ell_{A_t,t}$ of the chosen arm.
        \item \textbf{(Form importance-weighted loss)} For each arm $i$, define
        \[
            \tilde{\ell}_{i,t} \;=\;
            \begin{cases}
            K \,\ell_{A_t,t} & \text{if } i = A_t, \\
            0 & \text{otherwise}.
            \end{cases}
        \]
        \item \textbf{(Update weights)} For each arm $i$,
        \[
            w_i(t+1) \;=\; w_i(t) \;\exp\bigl(-\eta\,\tilde{\ell}_{i,t}\bigr),
        \]
        where $\eta > 0$ is a learning rate to be chosen.
    \end{enumerate}
\end{enumerate}

\bigskip
\noindent
\textbf{Expected Regret Bound (Sketch):} 

Let $L_i$ be the true cumulative loss of arm $i$ over $T$ rounds, and let $L^* = \min_i L_i$ be the cumulative loss of the best arm in hindsight. 
Using standard EXP3 analysis (with partial feedback), one can show that the expected regret, 
\[
    \mathbb{E}\bigl[\sum_{t=1}^T \ell_{A_t,t}\bigr] - L^*,
\]
is bounded by 
\[
    O\Bigl(\sqrt{K\,T \ln K}\Bigr),
\]
up to constant factors and the choice of $\eta$. 
Since the logging policy is uniform, the factor $K$ appears in the importance-weighted updates, 
but it is constant and known, which simplifies the analysis. 
With a more refined argument that accounts for small-variance assumptions (e.g., exploiting tighter concentration bounds for low-variance losses), 
it is possible to improve these guarantees further. 
In essence, if the true variance of the losses is small, one can tighten the deviation bounds on $\tilde{\ell}_{i,t}$, 
leading to smaller confidence terms and an improved regret rate.

\noindent
\subsubsection*{(d) Anytime Modification of EXP3}

\smallskip
For the anytime version of EXP3 (one that does not assume knowledge of a fixed horizon $T$), 
we replace the constant learning rate with one that depends on the current round $t$. 
A common choice is:
\[
    \eta_t \;=\; \sqrt{\frac{2\,\ln K}{K\,t}}.
\]
With this time-varying learning rate, the expected regret bound remains of the same order as the fixed-horizon version, namely
\[
    O\Bigl(\sqrt{K\,T\,\ln K}\Bigr),
\]
but with a slightly larger constant factor than if $T$ were known in advance.