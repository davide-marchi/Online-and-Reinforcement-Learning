
\section*{Exercise 1: PPO (33 points) [Christian]}

\subsection*{1.1 Return expressed as advantage over another policy (7 points)}

We wish to prove that the expected return of a policy $\pi$ can be written as
\begin{equation}
  J(\pi) = J(\pi_{\text{ref}}) + \mathbb{E}_{s_0\sim p_0,\pi} \left[ \sum_{t=0}^{\infty} \gamma^t \, A^{\pi_{\text{ref}}}(s_t,a_t) \right],
  \label{eq:goal}
\end{equation}
where the advantage function is defined by
\[
  A^{\pi_{\text{ref}}}(s,a) = Q^{\pi_{\text{ref}}}(s,a) - V^{\pi_{\text{ref}}}(s).
\]

\textbf{Proof:} Recall that for any state $s$ the state-value function of $\pi_{\text{ref}}$ is given by
\[
  V^{\pi_{\text{ref}}}(s) = \mathbb{E}_{a\sim\pi_{\text{ref}}} \left[ Q^{\pi_{\text{ref}}}(s,a) \right].
\]
Consider a trajectory $\tau=(s_0,a_0,s_1,a_1,\dots)$ generated by following $\pi$. For any finite horizon $T$, we can write
\begin{align*}
  \sum_{t=0}^{T} \gamma^t A^{\pi_{\text{ref}}}(s_t,a_t)
  &= \sum_{t=0}^{T} \gamma^t \Bigl( Q^{\pi_{\text{ref}}}(s_t,a_t) - V^{\pi_{\text{ref}}}(s_t) \Bigr)\\[1mm]
  &= \sum_{t=0}^{T} \gamma^t \left( r(s_t,a_t) + \gamma\, V^{\pi_{\text{ref}}}(s_{t+1}) - V^{\pi_{\text{ref}}}(s_t) \right).
\end{align*}
Notice that the sum telescopes. To see this, rearrange the terms:
\[
  \sum_{t=0}^{T} \gamma^t r(s_t,a_t)
  + \sum_{t=0}^{T} \left(\gamma^{t+1} V^{\pi_{\text{ref}}}(s_{t+1}) - \gamma^t V^{\pi_{\text{ref}}}(s_t) \right).
\]
The second sum is telescopic:
\[
  \sum_{t=0}^{T} \left(\gamma^{t+1} V^{\pi_{\text{ref}}}(s_{t+1}) - \gamma^t V^{\pi_{\text{ref}}}(s_t) \right)
  = - V^{\pi_{\text{ref}}}(s_0) + \gamma^{T+1} V^{\pi_{\text{ref}}}(s_{T+1}).
\]
Assuming that $V^{\pi_{\text{ref}}}(s)$ is bounded and $\gamma \in (0,1)$, as $T\to\infty$ we have $\gamma^{T+1} V^{\pi_{\text{ref}}}(s_{T+1})\to 0$. Therefore,
\[
  \sum_{t=0}^{\infty} \gamma^t A^{\pi_{\text{ref}}}(s_t,a_t)
  = \sum_{t=0}^{\infty} \gamma^t r(s_t,a_t) - V^{\pi_{\text{ref}}}(s_0).
\]
Taking the expectation over trajectories starting from $s_0 \sim p_0$ (and using the definition $J(\pi)=\mathbb{E}[\sum_{t\ge0}\gamma^t r(s_t,a_t)]$) gives
\[
  J(\pi) = \mathbb{E}_{s_0\sim p_0,\pi} \left[ \sum_{t=0}^{\infty} \gamma^t A^{\pi_{\text{ref}}}(s_t,a_t) \right] + \mathbb{E}_{s_0\sim p_0}\Bigl[ V^{\pi_{\text{ref}}}(s_0) \Bigr].
\]
Since by definition $J(\pi_{\text{ref}})=\mathbb{E}_{s_0\sim p_0}\left[V^{\pi_{\text{ref}}}(s_0)\right]$, we obtain the desired result (\ref{eq:goal}).

\vspace{2mm}

\subsection*{1.2 Clipping (13 points)}

In PPO the surrogate objective for a given state-action pair is defined as
\[
  L^{\text{CLIP}}(\theta) = \min\Biggl( \; r(\theta)\,\hat{A}^{\pi_{\text{ref}}}(s,a), \;
  \text{clip}\Bigl( r(\theta),\, 1-\epsilon,\, 1+\epsilon \Bigr) \, \hat{A}^{\pi_{\text{ref}}}(s,a) \Biggr),
\]
where
\[
  r(\theta) = \frac{\pi_{\theta}(a|s)}{\pi_{\text{ref}}(a|s)}
\]
and the clipping function is defined by
\[
  \text{clip}(x,\, l,\, u) = \min\{\max(x,l),u\}.
\]

\textbf{Intuitive discussion:} The gradient update will change $\pi_{\theta}(a|s)$ (and hence $r(\theta)$) if either 
\begin{enumerate}
  \item[(i)] $r(\theta) \in [1-\epsilon,\,1+\epsilon]$, or 
  \item[(ii)] when $r(\theta) \notin [1-\epsilon,\,1+\epsilon]$, the \emph{unclipped} term $r(\theta)\,\hat{A}^{\pi_{\text{ref}}}(s,a)$ has a gradient that \emph{points toward} the interval $[1-\epsilon,\,1+\epsilon]$.
\end{enumerate}
The first condition is trivial. Now assume that $r(\theta)\notin[1-\epsilon,1+\epsilon]$. There are two cases:

\textbf{Case 1:} $r(\theta)>1+\epsilon$. Then
\[
  \text{clip}(r(\theta),1-\epsilon,1+\epsilon) = 1+\epsilon.
\]
Now, consider the sign of $\hat{A}^{\pi_{\text{ref}}}(s,a)$:
\begin{itemize}
  \item If $\hat{A}^{\pi_{\text{ref}}}(s,a)>0$, then 
  \[
    r(\theta)\,\hat{A}^{\pi_{\text{ref}}}(s,a) > (1+\epsilon)\,\hat{A}^{\pi_{\text{ref}}}(s,a).
  \]
  Thus, the minimum in the objective is the clipped term, which is constant with respect to $\theta$ (i.e., its gradient is zero).
  
  \item If $\hat{A}^{\pi_{\text{ref}}}(s,a)<0$, then 
  \[
    r(\theta)\,\hat{A}^{\pi_{\text{ref}}}(s,a) < (1+\epsilon)\,\hat{A}^{\pi_{\text{ref}}}(s,a),
  \]
  so the unclipped term is active. Its gradient with respect to $\theta$ is proportional to 
  \[
    \nabla_\theta \Bigl[ r(\theta)\,\hat{A}^{\pi_{\text{ref}}}(s,a) \Bigr] = \hat{A}^{\pi_{\text{ref}}}(s,a) \nabla_\theta r(\theta).
  \]
  Since $\hat{A}^{\pi_{\text{ref}}}(s,a) < 0$, the gradient will be \emph{negative} (assuming $\nabla_\theta r(\theta)>0$ for an increase in $r(\theta)$), which implies that the update will \emph{decrease} $r(\theta)$ --- that is, it pushes $r(\theta)$ \emph{toward} the boundary $1+\epsilon$ rather than away from it.
\end{itemize}

\textbf{Case 2:} $r(\theta)<1-\epsilon$. By a similar argument, if $\hat{A}^{\pi_{\text{ref}}}(s,a)>0$, then the gradient of the unclipped term (which is active in this case) is positive and pushes $r(\theta)$ upward toward $1-\epsilon$.

\textbf{Formalization:} Assume that $r(\theta)\notin[1-\epsilon,1+\epsilon]$. Then:
\[
\begin{cases}
  r(\theta) > 1+\epsilon \quad \text{and} \quad \hat{A}^{\pi_{\text{ref}}}(s,a)<0,\\[1mm]
  r(\theta) < 1-\epsilon \quad \text{and} \quad \hat{A}^{\pi_{\text{ref}}}(s,a)>0.
\end{cases}
\]
In these cases, the derivative of the unclipped objective is
\[
  \nabla_\theta \left[ r(\theta)\,\hat{A}^{\pi_{\text{ref}}}(s,a) \right] = \hat{A}^{\pi_{\text{ref}}}(s,a)\, \nabla_\theta r(\theta).
\]
Thus, when $\hat{A}^{\pi_{\text{ref}}}(s,a)$ has the sign that makes this derivative point toward the interval, the gradient-based update will reduce the deviation of $r(\theta)$ from $[1-\epsilon,1+\epsilon]$. In other words, even if the current ratio is outside the interval, the gradient direction (if it does not point away from the interval) will drive it closer to the interval, ensuring that the policy update is conservative.

\vspace{2mm}

\subsection*{1.3 $\pi'$ in PPO (13 points)}

In the \emph{Gather experience} phase, the policy that generates the data is $\pi$ with parameters $\theta'$ (denoted as the behavior policy), and the probability of taking action $a_t^e$ in state $s_t^e$ is stored as 
\[
  p_t^e = \pi_{\theta'}(a_t^e|s_t^e).
\]
Later, during the PPO update the current policy $\pi_{\theta}$ (with updated parameters $\theta$) is used. Thus, the ratio
\[
  \frac{\pi_{\theta}(a_t^e|s_t^e)}{p_t^e} = \frac{\pi_{\theta}(a_t^e|s_t^e)}{\pi_{\theta'}(a_t^e|s_t^e)}
\]
generally differs from 1 because $\theta \neq \theta'$ in general. In other words, since the policy is updated over time, the probability of taking the same action in the same state under the current policy is typically not equal to the probability under the behavior policy that generated the data. This ratio is used as an importance sampling correction to account for the discrepancy between the two policies.
