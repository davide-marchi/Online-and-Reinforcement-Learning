\subsection{Neural architecture}

\subsubsection{Structure}
The line
\begin{lstlisting}[language=Python]
    x = torch.cat((x_input, x), dim=1)
\end{lstlisting}
takes the original input \(\texttt{x\_input}\) (the raw state features) and concatenates it with the hidden representation \(\texttt{x}\) along the feature dimension. This effectively merges the initial state features with the learned features from the hidden layers. Such a design is sometimes referred to as a \emph{skip connection} or \emph{residual-like} connection, because the network has direct access to the original input when producing the final Q-values.

\subsubsection{Activation function}
We use \(\tanh\) instead of the standard logistic sigmoid function because:
\begin{itemize}
  \item \(\tanh\) is zero-centered (ranging from \(-1\) to \(1\)), which often helps with training stability.
  \item The logistic (sigmoid) function saturates more easily at 0 or 1, leading to slower gradients. In contrast, \(\tanh\) keeps activations in a range that often accelerates convergence in practice.
\end{itemize}

\subsection{Adding the Q-learning}
The missing line to compute the target \(\mathbf{y}\) (right after the comment ``\texttt{\# Compute targets}'') is shown below. We also detach the tensor so that the gradient does not flow back through the next-state values:

\begin{lstlisting}[language=Python]
    max_elements = torch.max(target_Qs, dim=1)[0].detach()
    y = rewards + gamma * max_elements
\end{lstlisting}

This implements the standard Q-learning target:
\[
y_i \;=\; r_i \;+\; \gamma \,\max_{a'}Q(s'_i,\;a').
\]
After adding this line, the training converged to policies achieving returns above 200 in many episodes, indicating that the agent successfully learned to land.

\subsubsection*{Learning curve}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.65\textwidth]{Code/output.png}
  \caption{Accumulated reward per episode (in gray) and its smoothed average (blue).}
\end{figure}

\subsection{Epsilon}
The line
\begin{lstlisting}[language=Python]
    explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*ep)
\end{lstlisting}
implements an exponentially decaying exploration probability. At the beginning (when \(\texttt{ep} = 0\)), it starts near \(\texttt{explore\_start}\), and as the episode index \(\texttt{ep}\) grows, the probability \(\texttt{explore\_p}\) exponentially approaches \(\texttt{explore\_stop}\). This ensures that early in training the agent explores more, and then it gradually exploits its learned policy more often.

\subsection{Gather}
The line
\begin{lstlisting}[language=Python]
    Q_tensor = torch.gather(output_tensor, 1, actions_tensor.unsqueeze(-1)).squeeze()
\end{lstlisting}
selects the Q-value corresponding to the action actually taken in each state. Since \(\texttt{output\_tensor}\) contains Q-values for all possible actions, we use \(\texttt{gather}\) along the action dimension to pick out the one Q-value that corresponds to the \(\texttt{actions\_tensor}\). In other words, it is a convenient way to index a batch of Q-value vectors by their chosen actions.

\subsection{Target network}

\subsubsection*{Code modifications}
Below is the code I introduced to maintain a target network and perform Polyak averaging. First, I created \texttt{targetQN} right after creating \texttt{mainQN} and copied the parameters:
\begin{lstlisting}[language=Python]
# Create the target network
targetQN = QNetwork(hidden_size=hidden_size)

# Copy parameters from mainQN to targetQN so they start identical
targetQN.load_state_dict(mainQN.state_dict())
\end{lstlisting}

Then, at the end of each training step (i.e., right after \texttt{optimizer.step()}), I inserted the Polyak update:
\begin{lstlisting}[language=Python]
with torch.no_grad():
    for target_param, main_param in zip(targetQN.parameters(), mainQN.parameters()):
        target_param.data.copy_(tau * main_param.data + (1.0 - tau) * target_param.data)
\end{lstlisting}

This implements
\[
\theta_{\text{target}} \leftarrow \tau \,\theta_{\text{main}} \;+\; (1 - \tau)\,\theta_{\text{target}},
\]
where \(\tau \in (0,1)\) is the blend factor. 

The changes corrctly introduce a delayed target Q-network. We copy the main network's parameters once at initialization, then repeatedly blend them after every gradient update. This ensures that the target network changes slowly, providing more stable target values in the Q-learning loss.

\subsubsection*{Comparison with and without the target network}
I kept the same hyperparameters as before and introduced the target network. The learning curve is shown below.

\begin{figure}[H]
  \centering
  % The user can insert the final figure here:
  \includegraphics[width=0.65\textwidth]{Code/outputTargetQN.png}
  \caption{Accumulated reward per episode (in gray) and its smoothed average (blue) using a target network.}
\end{figure}

Empirically, we can see that the performance still reaches high returns, and in many cases the learning appears somewhat more stable. Although there can be noise and variability in individual runs, a target network typically reduces training instabilities and leads to more consistent convergence.