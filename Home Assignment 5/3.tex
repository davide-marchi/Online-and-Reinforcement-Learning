In adversarial settings, an algorithm’s performance is evaluated under the assumption that an adversary will create the most difficult possible scenarios. While we can try to test algorithms experimentally, it’s impossible to cover every possible adversarial strategy.

One key issue is that showing an algorithm is not robust is relatively easy. If we can find or construct an input where the algorithm fails, that’s enough to prove it has weaknesses. However, proving that an algorithm is robust is much harder. This would require showing that the algorithm works against every possible attack, but the space of adversarial strategies is often too large to test exhaustively.

A few challenges make a full evaluation difficult:

\begin{itemize}
    \item \textbf{Unlimited adversarial strategies:} Adversaries can design attacks based on how the algorithm works, and there’s no way to test for all possible attacks.
    \item \textbf{Adversaries can adapt over time:} Especially in real-world scenarios attackers constantly change their strategies, meaning any static test will only capture a limited picture.
    \item \textbf{Resource constraints:} Running large-scale experiments that simulate adversarial behavior is often too expensive and time-consuming to be practical.
\end{itemize}
    
\subsection*{Conclusion}

Since the space of possible adversarial attacks is too large and adaptive, no set of experiments can fully prove an algorithm is robust. Empirical testing can reveal weaknesses, but it can never provide a guarantee that an algorithm will hold up against every possible adversarial scenario.