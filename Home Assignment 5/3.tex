In adversarial settings, an algorithm's performance is judged under the assumption that an adversary can (and will) craft the most challenging possible scenarios. While we can \emph{attempt} to evaluate algorithms experimentally, there is no definitive way to exhaustively test every possible adversarial strategy. In particular:

\begin{itemize}
    \item \textbf{Showing that an algorithm is not robust is easy:} 
    A single experiment where the algorithm fails under a particular adversarial construction is enough to demonstrate a weakness. If we can find or generate inputs on which the algorithm performs poorly, that experiment shows the algorithm is not robust against that type of attack.

    \item \textbf{Showing that an algorithm is robust is very hard:}
    To conclusively prove robustness, one would have to show that the algorithm can handle \emph{all} adversarial strategies. However, the space of potential adversaries is typically enormous or even unbounded. Hence, no finite set of experiments can cover all possible attacks.

\end{itemize}

\paragraph{Why a comprehensive evaluation is difficult:}
\begin{enumerate}
    \item \textbf{Unlimited adversarial strategies:} 
    Adversaries can adapt based on the algorithmâ€™s structure or observed behavior. Designing experiments to capture \emph{all} potential strategies would require an exhaustive search, which is usually infeasible.

    \item \textbf{Dynamic nature of adversaries:}
    In many real-world scenarios (e.g., security, spam filtering, or adversarial machine learning), adversaries change their strategies over time in response to the system's defenses. This adaptive component makes static experiments only partially informative.

    \item \textbf{Experimental limitations:}
    Even if we focus on a specific type of adversarial behavior, constructing large-scale experiments that simulate adversarial conditions can be extremely resource-intensive (time, data, and computation).
\end{enumerate}

\subsection*{Conclusion}

\noindent It is not possible to fully certify that an algorithm is robust against all adversarial scenarios through finite experiments. Due to the vast space of potential adversarial attacks and their adaptive nature, no experimental design can guarantee absolute robustness in every possible adversarial environment. Therefore, empirical evaluation can show an algorithm \emph{is not} robust but cannot conclusively prove \emph{that it is} robust under all adversarial conditions.

