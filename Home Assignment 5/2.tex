\textbf{1(a). Apply Hoeffding's Lemma in the Hedge Analysis.}

\smallskip

We consider the Hedge algorithm, where at each round $t=1,\dots,T$:

\begin{itemize}
\item We have $K$ experts and maintain a cumulative loss $L_{t-1}(a)$ for each expert $a$.
\item We form a distribution 
\[
p_t(a) \;=\; \frac{\exp\bigl(-\eta\,L_{t-1}(a)\bigr)}{\sum_{a'} \exp\bigl(-\eta\,L_{t-1}(a')\bigr)}.
\]
\item We then observe the losses $\{\ell_t(a)\}_{a=1}^K$ (full information), and update $L_t(a) = L_{t-1}(a) + \ell_t(a)$.
\end{itemize}

Define
\[
W_t \;=\; \sum_{a=1}^K \exp\bigl(-\eta\,L_t(a)\bigr).
\]
Initially, $W_0 = K$ because $L_0(a) = 0$ for all $a$. 

To bound the regret, we look at 
\[
\sum_{a} p_t(a)\, e^{-\eta\,\ell_t(a)}
\;=\;
\mathbb{E}_{A \sim p_t}\Bigl[e^{-\eta\,\ell_t(A)}\Bigr].
\]
Here, $\ell_t(A)\in[0,1]$ (assuming losses are bounded in $[0,1]$). We apply Hoeffding's Lemma with $\alpha = -\eta < 0$. Then:
\[
\ln\Bigl(\mathbb{E}\bigl[e^{-\eta\,\ell_t(A)}\bigr]\Bigr)
\;\le\;
-\,\eta\;\mathbb{E}\bigl[\ell_t(A)\bigr]
\;+\;
\tfrac{\eta^2}{8}.
\]
Since $\mathbb{E}[\ell_t(A)] = \sum_{a} p_t(a)\,\ell_t(a)$, we get
\[
\ln\!\Bigl(\sum_{a} p_t(a)\, e^{-\eta\,\ell_t(a)}\Bigr)
\;\le\;
-\,\eta\,\sum_{a} p_t(a)\,\ell_t(a) \;+\; \frac{\eta^2}{8}.
\]

\bigskip

\noindent
\textbf{1(b). Find the $\eta$ that Minimizes the New Bound.}

\smallskip

Summing the above inequality over $t=1,\dots,T$ yields:
\[
\sum_{t=1}^T
\ln\Bigl(\sum_{a} p_t(a)\, e^{-\eta\,\ell_t(a)}\Bigr)
\;\le\;
-\,\eta \sum_{t=1}^T \sum_{a} p_t(a)\,\ell_t(a)
\;+\;
\frac{\eta^2\,T}{8}.
\]
On the other hand, from the definition of $W_T$ we have
\[
\frac{W_T}{W_0}
\;=\;
\prod_{t=1}^T \sum_{a} p_t(a)\, e^{-\eta\,\ell_t(a)}
\;\ge\;
\frac{1}{K}\,\exp\!\Bigl(-\,\eta\;\min_{a} L_T(a)\Bigr).
\]
Taking the logarithm and rearranging gives:
\[
-\,\frac{1}{\eta}\;
\sum_{t=1}^T
\ln\Bigl(\sum_{a} p_t(a)\, e^{-\eta\,\ell_t(a)}\Bigr)
\;\ge\;
\min_{a} L_T(a)
\;-\;
\frac{\ln(K)}{\eta}.
\]
Combine this with the previous sum to bound the \emph{regret}:
\[
\sum_{t=1}^T \sum_{a} p_t(a)\,\ell_t(a) \;-\; \min_{a} L_T(a)
\;\le\;
\frac{\ln(K)}{\eta} \;+\; \frac{\eta\,T}{8}.
\]
Denote the regret by $R_T$. We see
\[
R_T 
\;\le\;
\frac{\ln(K)}{\eta}
\;+\;
\frac{\eta\,T}{8}.
\]
To minimize the right-hand side, we set
\[
\frac{d}{d\eta}\Bigl(\tfrac{\ln(K)}{\eta} + \tfrac{\eta\,T}{8}\Bigr)
\;=\;
-\,\frac{\ln(K)}{\eta^2} + \frac{T}{8}
\;=\;0
\quad\Longrightarrow\quad
\eta
\;=\;
\sqrt{\frac{8\,\ln(K)}{T}}.
\]

\bigskip

\noindent
\textbf{1(c). Final Regret Bound.}

\smallskip

Plugging $\eta = \sqrt{\frac{8\,\ln(K)}{T}}$ back into the bound, we get
\[
R_T
\;\le\;
\ln(K)\,\sqrt{\frac{T}{8\,\ln(K)}} \;+\; \sqrt{\frac{8\,\ln(K)}{T}}\,\frac{T}{8}
\;=\;
2\,\sqrt{\frac{T\,\ln(K)}{8}}
\;=\;
\sqrt{\frac{T\,\ln(K)}{2}}.
\]
So the regret of Hedge can be bounded by
\[
\boxed{
R_T 
\;\le\;
\sqrt{\frac{T\,\ln(K)}{2}}
\,.
}
\]
This shows a sharper constant than the usual bound $O(\sqrt{T \ln K})$, thanks to applying Hoeffding's Lemma directly.

\bigskip

\noindent
\textbf{2. Why the Same Approach Cannot Be Used to Tighten the Regret Bound for EXP3.}

\smallskip

In the EXP3 algorithm, which deals with \emph{bandit} feedback, we do \emph{not} get to observe the losses $\ell_t(a)$ of all experts $a$ at each round. Instead, we only see the loss of the action we actually chose. Therefore, to update our weights, we rely on an \emph{unbiased but potentially high-variance estimator} of $\ell_t(a)$ for the unchosen actions. 

Hoeffding's Lemma in the form we used above requires direct access to the true $\ell_t(a)$ for the expectation inside the exponential. In EXP3, the random variable inside the exponential is replaced by the \emph{estimated} loss, which typically has higher variance and does not stay nicely within $[0,1]$ without additional complications. Consequently, we cannot directly apply the same sub-Gaussian/Hoeffding-type bound to get the improved constant. The partial information setting forces us to handle variance in the importance-weighted estimators, and that prevents us from achieving the same neat constant factor that we get in the full-information Hedge analysis.