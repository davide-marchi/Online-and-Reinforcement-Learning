\section*{Solution to Exercise 5.7}

We divide our solution into two main parts, exactly as indicated in the exercise:

\subsection*{Part 1: A tighter analysis of the Hedge algorithm}

\noindent
\textbf{Structure:} We address items (a), (b), and (c) in turn.

\bigskip

\noindent
\textbf{1(a).~Apply Hoeffding's lemma (Lemma 2.6) in order to get a tighter parametrization and bound.}

\noindent
\emph{Setup:} Recall that the Hedge algorithm for $K$ experts proceeds in rounds $t=1,\dots,T$, maintaining a loss vector $L_{t-1}(\cdot) = \bigl(L_{t-1}(1),\dots,L_{t-1}(K)\bigr)$ from previous rounds, where
\[
L_{t-1}(a) \;=\; \sum_{\tau=1}^{t-1}\ell_{\tau,a}.
\]
It chooses the expert $A_t$ with probability
\[
p_t(a) \;=\;
\frac{\exp\bigl(-\eta\,L_{t-1}(a)\bigr)}{\sum_{b=1}^K \exp\bigl(-\eta\,L_{t-1}(b)\bigr)}.
\]
After seeing all losses $\{\ell_{t,a}\}_{a=1}^K$ at round~$t$, the algorithm updates $L_t(a)=L_{t-1}(a)+\ell_{t,a}$.

We define the potential function
\[
W_t \;=\; \sum_{a=1}^K \exp\bigl(-\eta\,L_t(a)\bigr).
\]
Observe that
\[
\frac{W_t}{W_{t-1}}
\;=\;\frac{\sum_{a=1}^K \exp\bigl(-\eta\,L_{t-1}(a)\bigr)\exp\bigl(-\eta\,\ell_{t,a}\bigr)}
{\sum_{b=1}^K \exp\bigl(-\eta\,L_{t-1}(b)\bigr)}
\;=\;\sum_{a=1}^K p_t(a)\,\exp\bigl(-\eta\,\ell_{t,a}\bigr).
\]
We want to apply \emph{Hoeffding's lemma} to the exponential $\exp(-\eta\,\ell_{t,a})$ in order to tighten the standard analysis. 

\paragraph{Application of Hoeffding's lemma.}
Since each loss $\ell_{t,a}\in [0,1]$, we use the elementary bound (which can be viewed as a direct consequence of Hoeffding's lemma for $x\in[0,1]$ and $\lambda=-\eta<0$):
\[
e^{-\eta\,\ell_{t,a}} \;\le\; 1 - \eta\,\ell_{t,a} \;+\;\frac{\eta^2\,\ell_{t,a}^2}{2}.
\]
Hence,
\[
\frac{W_t}{W_{t-1}}
\;=\;\sum_{a=1}^K p_t(a)\,e^{-\eta\,\ell_{t,a}}
\;\le\;
\sum_{a=1}^K p_t(a)\Bigl(1 - \eta\,\ell_{t,a} + \tfrac{\eta^2\,\ell_{t,a}^2}{2}\Bigr)
\;=\;
1 \;-\;\eta\sum_{a=1}^K p_t(a)\,\ell_{t,a}
\;+\;\frac{\eta^2}{2}\sum_{a=1}^K p_t(a)\,\ell_{t,a}^2.
\]
Taking logarithms and using $\ln(1 - x)\le -\,x$ for $x<1$, we get
\[
\ln\!\Bigl(\frac{W_t}{W_{t-1}}\Bigr)
\;\le\;
-\eta\sum_{a=1}^K p_t(a)\,\ell_{t,a}
\;+\;\frac{\eta^2}{2}\sum_{a=1}^K p_t(a)\,\ell_{t,a}^2.
\]
Summing from $t=1$ to $T$ yields
\[
\ln\!\Bigl(\frac{W_T}{W_0}\Bigr)
\;\le\;
-\eta\sum_{t=1}^T \sum_{a=1}^K p_t(a)\,\ell_{t,a}
\;+\;\frac{\eta^2}{2}\sum_{t=1}^T \sum_{a=1}^K p_t(a)\,\ell_{t,a}^2.
\]
Note that $W_0 = \sum_{a=1}^K \exp\bigl(-\eta\,L_0(a)\bigr) = K$, since $L_0(a)=0$ for all $a$.  Also, for any fixed $a^*\in\{1,\dots,K\}$,
\[
W_T
\;=\;\sum_{a=1}^K e^{-\eta\,L_T(a)}
\;\ge\;
e^{-\eta\,L_T(a^*)}.
\]
Hence,
\[
-\eta\,L_T(a^*)
\;\le\;
\ln\!\bigl(W_T\bigr)
\;\le\;
\ln(K)
\;-\;\eta\sum_{t=1}^T \sum_{a=1}^K p_t(a)\,\ell_{t,a}
\;+\;\frac{\eta^2}{2}\sum_{t=1}^T \sum_{a=1}^K p_t(a)\,\ell_{t,a}^2.
\]
Rearrange and divide by $\eta$:
\[
\sum_{t=1}^T\sum_{a=1}^K p_t(a)\,\ell_{t,a}
\;-\;L_T(a^*)
\;\le\;
\frac{\ln(K)}{\eta}
\;+\;\frac{\eta}{2}\sum_{t=1}^T \sum_{a=1}^K p_t(a)\,\ell_{t,a}^2.
\]
Thus, we have derived a tighter parameterization (the extra term is $\frac{\eta}{2}\sum p_t(a)\,\ell_{t,a}^2$ instead of $\frac{\eta\,T}{2}$ in the naive version).  This completes item (a).

\bigskip

\noindent
\textbf{1(b).~Find the value of $\eta$ that minimizes the upper bound and write the final bound on $E[R_T]$.}

\noindent
First, we interpret the left-hand side as the \emph{expected loss of Hedge minus the best expert's total loss}:
\[
\sum_{t=1}^T \sum_{a=1}^K p_t(a)\,\ell_{t,a}
\;-\;\min_{a} L_T(a).
\]
When we take the algorithm's randomization into account, the regret $R_T$ is
\[
R_T
\;=\;
\sum_{t=1}^T \ell_{t,A_t}
\;-\;\min_{a} L_T(a),
\]
and by linearity of expectation,
\[
\mathbb{E}[R_T]
\;=\;
\mathbb{E}\Bigl[\sum_{t=1}^T \ell_{t,A_t}\Bigr]
\;-\;\min_{a} L_T(a)
\;=\;
\sum_{t=1}^T \sum_{a=1}^K p_t(a)\,\ell_{t,a}
\;-\;\min_{a} L_T(a).
\]
Hence the above inequality implies
\[
\mathbb{E}[R_T]
\;\le\;
\frac{\ln(K)}{\eta}
\;+\;\frac{\eta}{2}\,\sum_{t=1}^T \sum_{a=1}^K p_t(a)\,\ell_{t,a}^2.
\]
Since $\ell_{t,a}\in[0,1]$, it follows that $\ell_{t,a}^2 \le \ell_{t,a}\le 1$, so 
\[
\sum_{t=1}^T \sum_{a=1}^K p_t(a)\,\ell_{t,a}^2
\;\le\;
T.
\]
Thus
\[
\mathbb{E}[R_T]
\;\le\;
\frac{\ln(K)}{\eta}
\;+\;\frac{\eta\,T}{2}.
\]
To find the optimal $\eta$ that minimizes $\frac{\ln(K)}{\eta} + \tfrac{\eta\,T}{2}$, we take the derivative in $\eta$ and set to zero:
\[
-\frac{\ln(K)}{\eta^2}
\;+\;\frac{T}{2}
\;=\;0
\quad\Longrightarrow\quad
\eta
\;=\;\sqrt{\frac{2\,\ln(K)}{T}}.
\]
Substitute back to get
\[
\mathbb{E}[R_T]
\;\le\;
\sqrt{2\,T\,\ln(K)}.
\]
Often, an even more refined constant emerges if we apply a sharper version of the exponential bound for negative $x$.  The final bound given by the exercise is 
\[
\mathbb{E}[R_T]
\;\le\;
\sqrt{\frac{1}{2}\,T\,\ln(K)},
\]
which is a factor of $\sqrt{2}$ better than the classic statement.  This matches item~(b).

\bigskip

\noindent
\textbf{1(c).~At the end, show that you get an improvement by a factor of 2.}

\noindent
Comparing the final result
\[
\mathbb{E}[R_T] \;\le\;\sqrt{\tfrac12\,T\,\ln(K)}
\]
to the more standard (less careful) analysis
\[
\mathbb{E}[R_T] \;\le\;\sqrt{2\,T\,\ln(K)},
\]
we see we have indeed improved the leading constant by a factor of 2.  The sharper inequality arises from applying Hoeffding's lemma (rather than a simpler linearization) to $e^{-\eta\,\ell_{t,a}}$.  That completes part~1 of the exercise.

\bigskip

\subsection*{Part 2: Why the same approach can tighten the regret for EXP3}
In the second part of the exercise, we are asked to explain why the same approach can be used to tighten the regret of the EXP3 algorithm.  Recall that EXP3 is the bandit version of Hedge, i.e.\ it also uses exponential weighting, but updates the weights based on an \emph{unbiased estimate} of the losses (since we only observe the loss of the chosen action).  The analysis still relies on bounding 
\[
\frac{W_t}{W_{t-1}}
\;=\;\sum_{a=1}^K p_t(a)\,\exp\!\bigl(-\eta\,\widehat{\ell}_{t,a}\bigr),
\]
where $\widehat{\ell}_{t,a}$ is the suitably constructed unbiased estimator of $\ell_{t,a}$.  Exactly as in Hedge, one can use Hoeffding's lemma to bound $e^{-\eta\,\widehat{\ell}_{t,a}}$ by $1 - \eta\,\widehat{\ell}_{t,a} + \frac{\eta^2\,\widehat{\ell}_{t,a}^2}{2}$.  This leads to an analogous improvement in the final regret constants for EXP3, compared to a simpler linear bounding approach.  In short, the entire argument is parallel: the only difference is that we replace $\ell_{t,a}$ by $\widehat{\ell}_{t,a}$ in the analysis, but the exponential step and the potential function $W_t$ are the same.  Hence we get the same improvement factor.

\bigskip

\noindent
\textbf{End of solution.}

