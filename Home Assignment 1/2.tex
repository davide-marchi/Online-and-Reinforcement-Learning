\subsection*{Setting}

\begin{itemize}
\item We have \textbf{two actions}, $a^*$ and $a$, with rewards in $[0,1]$.
\item The rewards for each action are \emph{i.i.d.\ across rounds}, and \emph{fully observed} each round.
\item Let $\mu(a)$ be the true expected reward of action $a$.  Without loss of generality, let
\[
  a^* \;=\;\arg\max_{a} \,\mu(a).
\]
\item Define the \emph{gap} of the suboptimal arm by
\[
  \Delta
  \;=\;
  \mu(a^*) - \mu(a)
  \;>\;0.
\]
\item We consider $T$ rounds, indexed by $t=1,2,\dots,T$.
\end{itemize}

\subsection*{Algorithm:\; Follow the Leader (FTL)}

\begin{itemize}
\item \textbf{Initialization:} In the very first round(s), you may pick any arm(s).  
\item \textbf{At each round $t \ge 2$:}
  \begin{enumerate}
  \item For each arm $b \in \{a,a^*\}$, compute its \emph{empirical mean} based on \emph{all} observed rewards of $b$ so far:
  \[
    \hat{\mu}_{t-1}(b)
    \;=\;
    \frac{1}{t-1}\;\sum_{s=1}^{t-1} X_s(b),
  \]
  where $X_s(b)$ is the reward observed for arm $b$ at round $s$.
  \item \emph{Choose} the arm whose empirical mean is larger:
  \[
    A_t \;=\; \arg\max_{b \in \{a,a^*\}} \;\hat{\mu}_{t-1}(b).
  \]
  \end{enumerate}
\end{itemize}
Because this is a \textbf{full-information} setting, we observe $X_t(a)$ and $X_t(a^*)$ for both arms at every round $t$, not just the chosen one.

\subsection*{Key Idea:\; Probability of Confusion}

Let us define the event
\[
  \bigl\{\text{``FTL picks the suboptimal arm $a$ at round $t$''}\bigr\}
  \quad\Longleftrightarrow\quad
  \bigl\{\,\hat{\mu}_{t-1}(a) \;\ge\;\hat{\mu}_{t-1}(a^*)\bigr\}.
\]
Because $\Delta = \mu(a^*) - \mu(a) > 0,$ the only way for $\hat{\mu}_{t-1}(a)$ to overtake $\hat{\mu}_{t-1}(a^*)$ is if
\[
   \bigl[\hat{\mu}_{t-1}(a) - \mu(a)\bigr]
   \;\;-\;\;
   \bigl[\hat{\mu}_{t-1}(a^*) - \mu(a^*)\bigr]
   \;\;\ge\;\;\Delta.
\]
By Hoeffding's inequality (for $[0,1]$-bounded i.i.d.\ samples),
\[
  \mathbb{P}\Bigl(\hat{\mu}_{n}(b) - \mu(b)\;\ge\;\epsilon\Bigr)
  \;\le\;
  \exp\bigl(-2\,n\,\epsilon^2\bigr),
\]
one sees that
\[
  \mathbb{P}\!\Bigl(\hat{\mu}_{t-1}(a)\;\ge\;\hat{\mu}_{t-1}(a^*)\Bigr)
  \;\le\;
  \exp\bigl(-c\,(t-1)\,\Delta^2\bigr)
  \quad
  \text{for some constant }c>0.
\]
We will sometimes refer to this as the \emph{probability of confusion} at round~$t$.

\subsection*{Bounding the Regret}

\paragraph{Regret definition.}
The (pseudo-)regret is
\[
  R_T
  \;=\;
  \sum_{t=1}^T
  \bigl[\mu(a^*) \;-\;\mu(A_t)\bigr].
\]
Since $A_t\in\{a,a^*\}$, the only time we incur regret $\Delta>0$ is precisely when $A_t=a$.  Thus,
\[
  R_T
  \;=\;
  \Delta \,\sum_{t=1}^T \mathbf{1}\{\text{FTL picks $a$ at round $t$}\}.
\]
Taking expectation,
\[
  \mathbb{E}[R_T]
  \;=\;
  \Delta \,\sum_{t=1}^T
  \mathbb{P}\!\bigl(A_t = a\bigr)
  \;=\;
  \Delta \,\sum_{t=1}^T
  \mathbb{P}\!\Bigl(\hat{\mu}_{t-1}(a)\;\ge\;\hat{\mu}_{t-1}(a^*)\Bigr).
\]
Using the exponential bound from above, define
\[
  \delta(t)
  \;=\;
  \exp\bigl(-c\,(t-1)\,\Delta^2\bigr).
\]
Hence,
\[
  \mathbb{E}[R_T]
  \;\le\;
  \Delta \,\sum_{t=1}^T \delta(t).
\]

\paragraph{Geometric series.}
Note that $\sum_{t=1}^\infty \exp(-c\,(t-1)\,\Delta^2)$ converges (it is a geometric series).  Concretely,
\[
  \sum_{t=1}^\infty
  \exp\bigl(-c\,(t-1)\,\Delta^2\bigr)
  \;=\;
  \frac{1}{1 - e^{-\,c\,\Delta^2}}
  \;<\;\infty.
\]
Hence there is a constant $C(\Delta)$ such that
\[
  \sum_{t=1}^T
  \exp\bigl(-c\,(t-1)\,\Delta^2\bigr)
  \;\le\;
  C(\Delta),
  \quad
  \text{independently of $T$.}
\]
Multiplying by $\Delta$,
\[
  \mathbb{E}[R_T]
  \;\le\;
  \Delta \,C(\Delta).
\]
That is, \emph{the total expected regret} remains \textbf{bounded} by a constant with respect to~$T$.

\subsection*{Final Statement of the Bound}

Because FTL eventually ``locks onto'' the better arm (and stays there with high probability), we conclude
\[
  R_T
  \;=\;
  O(1)
  \quad\text{as}\;T\to\infty.
\]
A more explicit expression for the constant bound is
\[
  \mathbb{E}[\,R_T\,]
  \;\le\;
  \Delta
  \;\frac{1}{1 - e^{-\,c\,\Delta^2}}
  \;\;=\;
  \frac{\Delta}{1 - e^{-\,c\,\Delta^2}},
\]
which is finite for every $\Delta>0$.  If there are multiple suboptimal arms $a=1,\dots,K-1$, one simply sums similar terms for each suboptimal $\Delta(a)$; the result is still a finite constant.

\subsection*{Comparison with the Bandit Setting}

In the professor's \emph{bandit} slides, one observes only the reward of the chosen arm each round and thus must \emph{explore} explicitly, leading to a regret growing like $\log T$.  Here, in \emph{full-information} feedback, both arms' rewards are seen every time, so $\hat{\mu}_t(a)$ and $\hat{\mu}_t(a^*)$ concentrate exponentially fast for \emph{both} arms.  As a result, FTL typically makes only a finite number of mistakes (suboptimal plays), yielding a \textbf{constant} regret bound.