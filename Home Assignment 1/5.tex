\section*{Formulation of the Robber-Police Game as an MDP}

\begin{enumerate}

  \item[\textbf{(i)}]
    \textbf{State and Action Spaces.} \\
    We define each state \(s\) as a 4-tuple:
    \[
      s \;=\; \bigl(r_{\text{agent}},\,c_{\text{agent}},\,r_{\text{police}},\,c_{\text{police}}\bigr),
    \]
    where
    \[
      r_{\text{agent}},\,r_{\text{police}} \;\in\;\{0,1,2,3,4,5\},
      \quad
      c_{\text{agent}},\,c_{\text{police}} \;\in\;\{0,1,2\}.
    \]
    Therefore, the total number of states is
    \[
      |S| \;=\; 6 \times 3 \,\times\, 6 \times 3 \;=\; 324.
    \]
    We define the initial state as
    \[
      s_{\text{init}} \;=\; (0,\,0,\,1,\,2).
    \]

    The agent has five possible actions at each step:
    \[
      A \;=\; \{\uparrow,\;\downarrow,\;\leftarrow,\;\rightarrow,\;\text{stay}\}.
    \]
    Hence, \(\lvert A \rvert = 5\).

  \item[\textbf{(ii)}]
    \textbf{Reward Function.} \\
    We define the per-step reward function \(R(s,a)\) as follows:
    \[
      R(s, a) \;=\;
      \begin{cases}
        100000, & \text{if the agent is on a bank and the police is not there,} \\
        -10000, & \text{if the agent and the police share the same coordinates,} \\
        0, & \text{otherwise.}
      \end{cases}
    \]
    We then apply the usual discounted return framework with discount factor
    \(\gamma \in (0,1)\).  As discussed in class, we do not explicitly encode \(\gamma\) into the states.

  \item[\textbf{(iii)}]
    \textbf{Transition Probabilities.} \\
    We now specify the transition kernel
    \[
      P\bigl(s' \mid s, a\bigr),
    \]
    focusing on the example where the agent is at \(\text{Bank 1}\) and the police is at \(\text{Bank 4}\).  In coordinates, let
    \[
      s \;=\; (\,0,\,0,\,0,\,2).
    \]
    (Here \((0,0)\) denotes Bank~1 in row~0, column~0, and \((0,2)\) denotes Bank~4 in row~0, column~2.)

    We assume that \emph{walls act as reflectors}.  Concretely, if either the agent or the police attempts to move outside the grid, they are moved in the opposite direction by the same distance.  For instance:
    \begin{itemize}
      \item If the agent (or police) is at row \(0\) and attempts to move \(\uparrow\), they end up at row \(1\).
      \item If at column \(0\) and an action tries to move \(\leftarrow\), they end up at column \(1\).
      \item All other moves within the grid follow the usual row or column increment/decrement (bounded between row~\(0\) and row~\(5\), and column~\(0\) and column~\(2\)).
    \end{itemize}

    \paragraph{Agent's move.}
    From \(
    (r_{\text{agent}},c_{\text{agent}}) = (0,0)\),
    the next agent position \((r_{\text{agent}}',c_{\text{agent}}')\) is:
    \begin{itemize}
      \item \(\uparrow\):  attempts to go out of the grid (row~\(-1\)), then reflects and ends at \((1,0)\).
      \item \(\downarrow\): moves to \((1,0)\) normally (assuming row~\(0\) is top).
      \item \(\leftarrow\): attempts \((0,-1)\), reflects to \((0,1)\).
      \item \(\rightarrow\): moves to \((0,1)\) normally.
      \item \(\text{stay}\): remains at \((0,0)\).
    \end{itemize}

    \paragraph{Police's move.}
    Meanwhile, since the police is in the same row as the agent but to the right
    \(\bigl( (0,2) \text{ vs. } (0,0) \bigr)\), the problem statement tells us that it moves
    \(\uparrow\), \(\downarrow\), or \(\leftarrow\) with probability \(1/3\) each, again with reflection at the walls as described. Specifically:
    \begin{itemize}
      \item With probability \(\frac{1}{3}\): police tries "Up"; since it's at row 0, it bounces to row 1, yielding \((1,2)\).
      \item With probability \(\frac{1}{3}\): police tries "Down"; this is valid and leads to \((1,2)\).
      \item With probability \(\frac{1}{3}\): police tries "Left"; this moves it from \((0,2)\) to \((0,1)\).
    \end{itemize}

    \paragraph{Combining to get next-state probabilities.}
    \begin{itemize}
      \item \((1,2)\) with probability \(\frac{2}{3}\) (sum of Up/Down probabilities).
      \item \((0,1)\) with probability \(\frac{1}{3}\).
    \end{itemize}

    So the next state is:
    \begin{itemize}
      \item \((0,1,1,2)\) with probability \(\frac{2}{3}\).
      \item \((0,1,0,1)\) with probability \(\frac{1}{3}\).
    \end{itemize}

    One can similarly list the possible next states and their probabilities for the other agent actions.

\end{enumerate}

