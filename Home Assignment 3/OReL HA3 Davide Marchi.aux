\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Direct Policy Search}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Multi-variate normal distribution}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Neuroevolution}{4}{subsection.1.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Definition of the policy network.}{5}{lstlisting.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance comparison of policy networks with and without bias parameters.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:results}{{1}{6}{Performance comparison of policy networks with and without bias parameters}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Off-Policy Optimization in RiverSwim}{6}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Performance metrics in the original RiverSwim MDP: (left) $Q$-value error $\|Q^* - Q_t\|_\infty $, (middle) policy mismatch count, and (right) return loss at state 0 (action = right).}}{7}{figure.1}\protected@file@percent }
\newlabel{fig:original}{{1}{7}{Performance metrics in the original RiverSwim MDP: (left) $Q$-value error $\|Q^* - Q_t\|_\infty $, (middle) policy mismatch count, and (right) return loss at state 0 (action = right)}{figure.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Definition of the policy network.}{7}{lstlisting.2}\protected@file@percent }
\bibdata{bibliography}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance metrics in the variant RiverSwim MDP, where the reward for action = right in the rightmost state is drawn from $\mathrm  {Unif}[0,2]$.}}{8}{figure.2}\protected@file@percent }
\newlabel{fig:variant}{{2}{8}{Performance metrics in the variant RiverSwim MDP, where the reward for action = right in the rightmost state is drawn from $\mathrm {Unif}[0,2]$}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reward Shaping}{8}{section.3}\protected@file@percent }
\gdef \@abspage@last{8}
