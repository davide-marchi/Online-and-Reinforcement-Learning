{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMA-ES for CartPole Torch (assignment version)\n",
    "### Christian Igel, 2024\n",
    "\n",
    "If you have suggestions for improvement, [let me know](mailto:igel@diku.dk).\n",
    "\n",
    "You may need the following packages:\n",
    "\n",
    "``pip install gymnasium[classic-control]``\n",
    "\n",
    "``python -m pip install cma``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Defines RL environments\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (4,4)  # Set size of visualization\n",
    "from IPython.display import clear_output  # For inline visualization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cma\n",
    "\n",
    "# Define task\n",
    "env = gym.make('CartPole-v1')\n",
    "state_space_dimension = env.observation_space.shape[0]\n",
    "action_space_dimension = 1  # env.action_space.n - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the policy network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sum(\n",
    "\tparam.numel() for param in policy_net.parameters()\n",
    ")\n",
    "print(\"Number of parameters:\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(policy_net):\n",
    "    env_render = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    state = env_render.reset()  # Forget about previous episode\n",
    "    state_tensor = torch.Tensor( state[0].reshape((1, state_space_dimension)) )\n",
    "    steps = 0\n",
    "    while True:\n",
    "        out = policy_net(state_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env_render.step(a)  # Simulate pole\n",
    "        steps+=1\n",
    "        state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) )\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(env_render.render())\n",
    "        plt.show()\n",
    "        print(\"step:\", steps)\n",
    "        if(terminated or truncated): \n",
    "            break\n",
    "    env_render.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy(policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the objective/reward function. \n",
    "When the task is solved the functions returns -1000.\n",
    "One successful trial is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_cart_pole(x, nn, env):\n",
    "    '''\n",
    "    Returns negative accumulated reward for single pole, fully environment.\n",
    "\n",
    "    Parameters:\n",
    "        x: Parameter vector encoding the weights.\n",
    "        nn: Parameterized model.\n",
    "        env: Environment ('CartPole-v?').\n",
    "    '''\n",
    "    torch.nn.utils.vector_to_parameters(torch.Tensor(x), nn.parameters())  # Set the policy parameters\n",
    "    \n",
    "    state = env.reset()  # Forget about previous episode\n",
    "    state_tensor = torch.Tensor( state[0].reshape((1, state_space_dimension)) )\n",
    "          \n",
    "    R = 0  # Accumulated reward\n",
    "    while True:\n",
    "        out = nn(state_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env.step(a)  # Simulate pole\n",
    "        state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) )\n",
    "        R += reward  # Accumulate \n",
    "        if truncated:\n",
    "            return -1000  # Episode ended, final goal reached, we consider minimization\n",
    "        if terminated:\n",
    "            return -R  # Episode ended, we consider minimization\n",
    "    return -R  # Never reached  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate initial search point and initial hidden RNN states\n",
    "initial_weights = np.random.normal(0, 0.01, d)  # Random parameters for initial policy, d denotes the number of weights\n",
    "initial_sigma = .01 # Initial global step-size sigma\n",
    "\n",
    "# Do the optimization\n",
    "res = cma.fmin(fitness_cart_pole,  # Objective function\n",
    "               initial_weights,  # Initial search point\n",
    "               initial_sigma,  # Initial global step-size sigma\n",
    "               args=([policy_net, env]),  # Arguments passed to the fitness function\n",
    "               options={'ftarget': -999.9, 'tolflatfitness':1000, 'eval_final_mean':False})\n",
    "env.close()\n",
    "\n",
    "# Set the policy parameters to the final solution\n",
    "torch.nn.utils.vector_to_parameters(torch.Tensor(res[0]), policy_net.parameters())  \n",
    "\n",
    "print(\"best solution found after\", res[2], \"evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_policy(policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about CMA-ES optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimization \n",
    "#cma.plot();  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn even more on CMA-ES\n",
    "#cma.CMAOptions() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
