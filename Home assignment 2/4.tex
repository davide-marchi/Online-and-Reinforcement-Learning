\emph{All of the Python code used to run these experiments (i.e., PI, VI, and Anchored VI, 
plus the 4-room environment and the visualizations) can be found in the file 
\texttt{HA2\_gridworld.py}.}

\bigskip

\noindent
\textbf{Environment Setup:}\\
We have a \(7\times 7\) grid with walls, yielding 20 accessible states labeled
\(0\) to \(19\). State \(19\) (bottom-right corner) has reward \(1\) and 
is effectively absorbing once reached. The agent chooses from 4 compass actions 
(\(\mathtt{0}=\text{Up}\), \(\mathtt{1}=\text{Right}\), 
\(\mathtt{2}=\text{Down}\), \(\mathtt{3}=\text{Left}\)) but transitions are 
\emph{slippery}: with probability \(0.7\) it goes in the chosen direction, 
\(0.1\) each for perpendicular directions, and \(0.1\) for staying in place. 
We set \(\gamma=0.97\) by default, except in part (iii) where \(\gamma=0.998\).

\subsection*{(i) Solve the grid-world task using PI (Policy Iteration).}

\noindent
We implemented \emph{Policy Iteration}, which alternates:
\[
  \resizebox{\textwidth}{!}{
  $\text{Policy Iteration:}\quad
  \begin{cases}
    \text{(a) Evaluate current policy }\pi: 
      & V^\pi = (I - \gamma P^\pi)^{-1} \, r^\pi,\\
    \text{(b) Improve } \pi: & 
      \pi \; \leftarrow \; \arg\max_{a} 
        \Bigl[r(s,a)+\gamma\!\!\sum_{s'}\!P(s'\!\mid s,a)\,V^\pi(s')\Bigr].
  \end{cases}$
  }
\]
It converged in 4 iterations. The resulting \emph{optimal policy} (an array of size 20) is
\[
  \pi_{\mathrm{PI}} \;=\; 
  [\,1,\;1,\;1,\;2,\;2,\;2,\;0,\;2,\;3,\;2,\;2,\;1,\;1,\;1,\;1,\;2,\;1,\;0,\;1,\;0\,].
\]
The \emph{optimal value function} \(V^*(s)\), rounded to 2 decimals, is
\[
  \resizebox{\textwidth}{!}{
  $V^* \;=\;
  [\,23.07,\;23.97,\;25.15,\;26.26,\;25.40,\;23.97,\;23.07,\;27.71,\;26.40,\;25.15,\;
    29.12,\;26.26,\;27.71,\;29.12,\;30.40,\;31.74,\;25.40,\;26.40,\;31.74,\;33.33\,].$
  }
\]
Graphically, we can visualize \(\pi\) on the \(7\times 7\) grid as:
\[
\begin{array}{l}
\text{\ttfamily
[ [Wall,  Wall,  Wall,  Wall,  Wall,  Wall,  Wall],}\\
\quad \text{\ttfamily [Wall, Right, Right, Right, Down, Down, Wall],}\\
\quad \text{\ttfamily [Wall, Down,  Up,   Wall,  Down, Left,  Wall],}\\
\quad \text{\ttfamily [Wall, Down,  Wall,  Wall,  Down, Wall,  Wall],}\\
\quad \text{\ttfamily [Wall, Right, Right, Right, Right, Down, Wall],}\\
\quad \text{\ttfamily [Wall, Right, Up,   Wall,  Right, Up,   Wall],}\\
\quad \text{\ttfamily [Wall, Wall,  Wall,  Wall,  Wall, Wall,  Wall] ]}.
\end{array}
\]

\subsection*{(ii) Implement VI and use it to solve the grid-world task.}

\noindent
We then implemented \emph{Value Iteration}, repeatedly applying
\[
  V_{n+1}(s) \;=\;\max_{a}\Bigl[r(s,a)+\gamma\!\!\sum_{s'}\!P(s'\mid s,a)\,V_{n}(s')\Bigr].
\]
With \(\gamma=0.97\), VI converged in 48 iterations and recovered \emph{the same} 
optimal policy and value function as in part (i).  The same 7$\times$7 map of actions
applies.

\subsection*{(iii) Repeat (ii) with \(\gamma=0.998\).}

\noindent
Now we let \(\gamma=0.998\).  VI converged more slowly (56 iterations). 
The final value function is larger (in the 400--500 range). 
The \emph{policy map} is mostly similar, but with some minor differences in early states, 
reflecting the higher weighting of future rewards:
\[
\begin{array}{l}
\text{\ttfamily
[ [Wall,  Wall,  Wall,  Wall,  Wall,  Wall,  Wall],}\\
\quad \text{\ttfamily [Wall, Down,  Right, Right, Down,  Down,  Wall],}\\
\quad \text{\ttfamily [Wall, Down,  Left,  Wall, Down,  Left,  Wall],}\\
\quad \text{\ttfamily [Wall, Down,  Wall,  Wall, Down,  Wall,  Wall],}\\
\quad \text{\ttfamily [Wall, Right, Right, Right,Right, Down,  Wall],}\\
\quad \text{\ttfamily [Wall, Right, Up,    Wall,Right, Up,    Wall],}\\
\quad \text{\ttfamily [Wall, Wall,  Wall,  Wall,Wall,  Wall,  Wall] ]}.
\end{array}
\]

\subsection*{(iv) Anchored Value Iteration (Anc-VI).}

\noindent
We add an anchor \(V_0\) to VI with
\[
  V_{n+1}
  \;=\;\beta_{n+1}\,V_0 \;+\;\bigl(1-\beta_{n+1}\bigr)\,\max_{a}\Bigl[\dots\Bigr].
\]
We tested three anchors:
\begin{enumerate}[label=(\alph*)]
\item $V_0=0$,
\item $V_0=\mathbf{1}$,
\item $V_0$ randomly in $[\,0,\,1/(1-\gamma)\,]^{nS}$.
\end{enumerate}
Below are the iteration counts needed:
\begin{itemize}
  \item[(a)] \textbf{anchor=0:} 300 iterations
  \item[(b)] \textbf{anchor=1:} 300 iterations
  \item[(c)] \textbf{random anchor:} 284 iterations
\end{itemize}
The first two converge to a very similar optimal policy and value.  
\emph{However}, when using the randomized anchor the Anc-VI tends to converge to a suboptimal policy,
indicating that certain initializations can cause local numerical issues and not allow to change from
a suboptimal policy if the initialization is not good end they are "anchored" to it.

\subsection*{(v) Compare the convergence speed of VI vs.\ Anc-VI for the same anchors.}

\noindent
Finally, we compared standard VI ($\mathtt{anc}=\text{False}$) vs.\ anchored VI ($\mathtt{anc}=\text{True}$) 
for each of the three anchors:
\[
  \begin{array}{lccc}
  \toprule
  & \textbf{Anchor=0} & \textbf{Anchor=1} & \textbf{Anchor=Random} \\
  \midrule
  \textbf{Standard VI} & 592\,\text{iters} & 591\,\text{iters} & 543\,\text{iters} \\[5pt]
  \textbf{Anchored VI} & 300\,\text{iters} & 300\,\text{iters} & 284\,\text{iters} \\
  \bottomrule
  \end{array}
\]
Hence anchored VI took fewer iterations overall in our runs, whereas standard VI needed more 
(five to six hundred).  As noted, the random anchor can lead to suboptimal policies if used 
with the anchored update step, illustrating that initialization can matter.