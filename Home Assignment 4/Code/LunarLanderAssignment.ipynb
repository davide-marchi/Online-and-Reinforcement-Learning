{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander with REINFORCE\n",
    "### Christian Igel, 2023\n",
    "\n",
    "If you have suggestions for improvements, [let me know](mailto:igel@diku.dk).\n",
    "\n",
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from tqdm.notebook import tqdm, trange  # Progress bar\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need [the `gymnasium` package](https://gymnasium.farama.org/).\n",
    "From this package, we create the Cart-Pole game environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_visual = gym.make('LunarLander-v2', render_mode=\"human\")\n",
    "action_size = 4\n",
    "state_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test the environment first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 5\n",
    "for _ in range(test_episodes):\n",
    "    R = 0\n",
    "    state, _ = env_visual.reset()  # Environment starts in a random state, cart and pole are moving\n",
    "    print(\"initial state:\", state)\n",
    "    while True:  # Environment sets \"truncated\" to true after 500 steps \n",
    "        # Uncomment the line below to watch the simulation\n",
    "        env_visual.render()\n",
    "        state, reward, terminated, truncated, _ = env_visual.step(env_visual.action_space.sample()) #  Take a random action\n",
    "        R += reward  # Accumulate reward\n",
    "        if terminated or truncated:\n",
    "            print(\"return: \", R)\n",
    "            env_visual.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "Let's define a policy class for a simple softmax policy for real-valued feature vectors and discrete actions.\n",
    "The preference for an action is just a linear function of the input features.\n",
    "It is not trivial that this simple policy is powerful enough to solve the tasks without addional processing of the input features. However, it is indeed possible to get reasonable policies in this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_policy:\n",
    "    def __init__(self, no_actions, no_features):\n",
    "        \"\"\"\n",
    "        Initialize softmax policy for discrete actions\n",
    "        :param no_actions: number of actions\n",
    "        :param no_features: dimensionality of feature vector representing a state\n",
    "        \"\"\"        \n",
    "        self.no_actions = no_actions\n",
    "        self.no_features = no_features\n",
    "\n",
    "        # Initialize policy parameters to zero\n",
    "        self.theta = np.zeros([no_actions, no_features])\n",
    "        \n",
    "    def pi(self, s):\n",
    "        \"\"\"\n",
    "        Compute action probabilities in a given state\n",
    "        :param s: state feature vector\n",
    "        :return: an array of action probabilities\n",
    "        \"\"\"\n",
    "        # Compute action preferences for the given feature vector\n",
    "        preferences = self.theta.dot(s)\n",
    "        # Convert overflows to underflows\n",
    "        preferences = preferences - preferences.max()\n",
    "        # Convert the preferences into probabilities\n",
    "        exp_prefs = np.exp(preferences)\n",
    "        return exp_prefs / np.sum(exp_prefs)\n",
    "    \n",
    "    def inc(self, delta):\n",
    "        \"\"\"\n",
    "        Change the parameters by addition, e.g. for initialization or parameter updates \n",
    "        :param delta: values to be added to parameters\n",
    "        \"\"\"\n",
    "        self.theta += delta\n",
    "\n",
    "    def sample_action(self, s):\n",
    "        \"\"\"\n",
    "        Sample an action in a given state\n",
    "        :param s: state feature vector\n",
    "        :return: action\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.no_actions, p=self.pi(s))\n",
    "\n",
    "    def gradient_log_pi(self, s, a):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the logarithm of the policy\n",
    "        :param s: state feature vector\n",
    "        :param a: action\n",
    "        :return: gradient of the logarithm of the policy\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def gradient_log_pi_test(self, s, a, eps=0.1):\n",
    "        \"\"\"\n",
    "        Numerically approximates the gradient of the logarithm of the policy\n",
    "        :param s: state feature vector\n",
    "        :param a: action\n",
    "        :return: approximate gradient of the logarithm of the policy\n",
    "        \"\"\"\n",
    "        theta_correct = np.copy(self.theta)\n",
    "        log_pi = np.log(self.pi(s)[a])\n",
    "        d = np.zeros([self.no_actions, self.no_features])\n",
    "        for i in range(self.no_actions):\n",
    "            for j in range(self.no_features):\n",
    "                self.theta[i,j] += eps\n",
    "                log_pi_eps = np.log(self.pi(s)[a])\n",
    "                d[i,j] = (log_pi_eps - log_pi) / eps\n",
    "                self.theta = np.copy(theta_correct)\n",
    "        return d\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify gradient implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "s = env.reset()[0]\n",
    "pi = Softmax_policy(action_size, state_size)\n",
    "tolerance = 0.001  # Absolute tolerance for difference in each gradient component\n",
    "epsilon = 0.0001\n",
    "for _ in range(10):\n",
    "    pi.inc(10.*np.random.rand(action_size, state_size))\n",
    "    for a in range(action_size):\n",
    "        if not np.isclose(pi.gradient_log_pi(s, a), pi.gradient_log_pi_test(s, a, epsilon), atol=tolerance).all():\n",
    "            print(\"derivative test for action\", a)\n",
    "            print(pi.gradient_log_pi(s, a))\n",
    "            print(pi.gradient_log_pi_test(s, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.00005  # Learning rate\n",
    "\n",
    "no_episodes = 20000  # Number of episodes\n",
    "total_reward_list = []  # Returns for the individual episodes\n",
    "pi = Softmax_policy(action_size, state_size)  # Policy\n",
    "\n",
    "# Do the learning\n",
    "for e in trange(no_episodes):  #  Loop over episodes\n",
    "    R = []  # Store rewards r_1, ..., r_T\n",
    "    S = []  # Store actions a_0, ..., a_{T-1}\n",
    "    A = []  # Store states s_0, ..., s_{T-1}\n",
    "    state = env.reset()[0]  # Environment starts in a random state, cart and pole are moving\n",
    "    while True:  # Environment sets \"done\" to true after 200 steps \n",
    "        S.append(state)\n",
    "        \n",
    "        action = pi.sample_action(state)  # Take an action following pi\n",
    "        A.append(action)\n",
    "        \n",
    "        state, reward, terminated, truncated, _ = env.step(action)  # Observe reward and new state\n",
    "        R.append(reward)\n",
    "                \n",
    "        if terminated or truncated:  # Failed or succeeded?\n",
    "            break\n",
    "            \n",
    "    R = np.array(R)\n",
    "    total_reward_list.append((e, R.sum()))\n",
    "    \n",
    "    for t in range(R.size):\n",
    "        R_t = R[t:].sum()  # Accumulated future reward\n",
    "        Delta = alpha * R_t * pi.gradient_log_pi(S[t], A[t])  # REINFORCE update\n",
    "        pi.inc(Delta)  # Apply update\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average for smoothing plot\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, x[0]*np.ones(N)))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / N\n",
    "\n",
    "eps, rews = np.array(total_reward_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps, smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Accumulated Reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env_visual.reset()[0]  # Environment starts in a random state, cart and pole are moving\n",
    "R = 0\n",
    "while True:  # Environment sets \"truncated\" to true after 500 steps \n",
    "        env_visual.render()\n",
    "        state, reward, terminated, truncated, _ = env_visual.step( pi.sample_action(state) ) #  Take a  action\n",
    "        R += reward  # Accumulate reward\n",
    "        if terminated or truncated:\n",
    "            print(\"return: \", R)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OReL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
