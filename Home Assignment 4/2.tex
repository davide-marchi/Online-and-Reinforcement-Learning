We consider a scenario with two products:
\begin{itemize}
  \item \textbf{Old product:} Known success probability \(p_{\text{old}} = 0.5\).
  \item \textbf{New product:} Unknown success probability \(\mu\).
\end{itemize}
Define \(\Delta = 0.5 - \mu\). Our goal is to maximize the number of successes over \(T\) rounds by choosing, at each time \(t\), one product to offer.

\subsection*{Algorithm Description}

We adopt a two-phase strategy:
\begin{enumerate}
    \item \textbf{Exploration:} In the first \(n\) rounds (with \(n\) a small constant), choose the new product to obtain an empirical estimate
    \[
    \hat{\mu}_n = \frac{S_n}{n},
    \]
    where \(S_n\) is the number of successes in these \(n\) trials.
    
    \item \textbf{Exploitation:} For \(t > n\), choose between the old and new products based on a UCB-type index. Define:
    \[
    I_{\text{old}} = 0.5, \quad
    I_{\text{new}}(t) = \hat{\mu}_{t-1} + \sqrt{\frac{2\ln t}{N_{\text{new}}(t-1)}},
    \]
    where \(N_{\text{new}}(t-1)\) is the number of times the new product has been chosen up to time \(t-1\). At time \(t\), select the product with the higher index.
\end{enumerate}

\subsection*{Regret Definition}

The pseudo-regret is defined as
\[
R(T) = \mathbb{E}\left[T \max\{0.5, \mu\} - \sum_{t=1}^{T} r_t\right],
\]
where \(r_t\) is the reward at time \(t\). We analyze the two cases separately.

\subsection*{Case 1: \(\mu < 0.5\) (New Product is Suboptimal)}

In this case, the optimal product is the old one, and a regret of \(\Delta = 0.5-\mu\) is incurred each time the new product is selected. Let \(N_{\text{new}}(T)\) be the total number of times the new product is chosen up to time \(T\). Then,
\[
R(T) = \Delta\, \mathbb{E}\left[N_{\text{new}}(T)\right].
\]

With high probability, for all \(t > n\), the number of times the new product is selected satisfies
\[
\mathbb{E}\left[N_{\text{new}}(T)\right] \leq n + \frac{6 \ln T}{\Delta^2}.
\]

\textbf{Proof:}\\[1ex]
Using Hoeffding's inequality, for any \(t > n\) we have:
\[
\Pr\left(\hat{\mu}_{t-1} \geq \mu + \epsilon\right) \leq \exp\left(-2N_{\text{new}}(t-1)\epsilon^2\right).
\]
By setting
\[
\epsilon = \sqrt{\frac{2\ln t}{N_{\text{new}}(t-1)}},
\]
we ensure that, with high probability,
\[
\hat{\mu}_{t-1} \leq \mu + \sqrt{\frac{2\ln t}{N_{\text{new}}(t-1)}}.
\]
Thus, the index for the new product satisfies
\[
I_{\text{new}}(t) \leq \mu + 2\sqrt{\frac{2\ln t}{N_{\text{new}}(t-1)}}.
\]
The new product is selected only if
\[
I_{\text{new}}(t) \geq 0.5,
\]
which is equivalent to
\[
\mu + 2\sqrt{\frac{2\ln t}{N_{\text{new}}(t-1)}} \geq 0.5.
\]
Rearranging and solving for \(N_{\text{new}}(t-1)\) (and applying a union bound over \(t\)) yields
\[
N_{\text{new}}(T) \leq n + \frac{6\ln T}{\Delta^2},
\]
in expectation.

Thus, the pseudo-regret is bounded by
\[
R(T) \leq \Delta \left(n + \frac{6\ln T}{\Delta^2}\right) = n\Delta + \frac{6\ln T}{\Delta}.
\]

\subsection*{Case 2: \(\mu > 0.5\) (New Product is Optimal)}

When \(\mu > 0.5\), the new product is optimal. Regret is incurred only when the algorithm mistakenly selects the old product. Since the exploration phase forces the new product for \(n\) rounds and, after sufficient samples, the UCB index of the new product will, with high probability, exceed \(0.5\), the number of suboptimal choices is bounded by a constant independent of \(T\). Therefore, the pseudo-regret is
\[
R(T) = O(1).
\]

\subsection*{Conclusion}

Combining both cases, we conclude that the pseudo-regret satisfies
\[
R(T) =
\begin{cases}
O\left(\ln T\right), & \text{if } \mu < 0.5, \\
O(1), & \text{if } \mu > 0.5.
\end{cases}
\]